Bert Training
=========================

The task implements a BERT (Bidirectional Encoder Representations from Transformers) model for two primary tasks: Masked Language Modeling (MLM) and Next Sentence Prediction (NSP). BERT is a pre-trained Transformer-based model designed for various natural language understanding tasks.

1. Masked Language Modeling (MLM) Task:
Model Architecture:
The BERT model consists of multiple encoder layers, each containing a multi-head self-attention mechanism and a position-wise feedforward network.

2. Dataset and Data Generation:
Dataset:
The code uses a synthetic dataset generated by the make_batch() function.
The dataset is formatted with input sequences, segment indices, masked tokens, masked positions, and labels for the next sentence prediction.
Training Objective:
The model is trained to simultaneously perform two tasks:
Predict masked tokens in the input sequence (MLM task).
Predict whether the next sentence is a continuation of the current one (NSP task).
Conclusion:

Training a BERT model on a simple wiki dataset for a masked language modeling task and a next sentence prediction task. The model architecture includes multiple encoder layers, and the training process involves minimizing the combined loss of MLM and NSP using the Adam optimizer. The use of synthetic data allows for controlled experimentation and showcases the end-to-end training process for a BERT model on these specific natural language processing tasks.


Web Application
======================================
Flask web application for semantic search using a pre-trained sentence transformer model. 

The web app allows users to input a query through an HTML form.
On form submission, the entered query is processed in the backend.
Pre-trained Sentence Transformer Model:

The code uses the "all-MiniLM-L6-v2" pre-trained sentence transformer model.
This model is capable of encoding sentences into numerical representations that capture semantic similarities.
Data Loading and Corpus Creation:

The web app loads a sample dataset from a Parquet file ("0000.parquet") using Pandas.
The function get_corpus() extracts a limited number of sentences from the dataset to form a corpus.
Cosine Similarity Calculation:

A function named get_cosine_similarity(query, corpus) calculates the cosine similarity between the user's query and each sentence in the corpus.
The function uses the sentence transformer model to encode both the query and the corpus.
Web Interface:

The web interface includes an HTML form with an input box for users to enter their query.
Upon submitting the query, the backend calculates the cosine similarity scores between the query and sentences in the corpus.
The most similar sentence and its similarity score are displayed to the user.
Result Presentation:

The result is presented in the same HTML page, indicating the similarity score and the most similar sentence from the corpus.
Running the Web App:
The Flask application is run with app.run(debug=True).
Users can access the web app through a web browser, and the interface allows them to input queries for semantic search.
Conclusion:
In summary, the web application provides a user-friendly interface for semantic search using a pre-trained sentence transformer model. It loads a sample dataset, creates a corpus, and calculates the cosine similarity between user queries and sentences in the corpus. The results are then presented to the user through the web interface. This application showcases the integration of natural language processing capabilities for semantic search within a Flask web framework.

* Webpage
![alt text](./app/static/app-page.png?raw=true)
